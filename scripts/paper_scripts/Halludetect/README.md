# Hallucination Detection (HalluDetect) Scripts

This folder contains scripts to calculate and evaluate uncertainty-based features for hallucination detection, following the **HalluDetect** methodology. These features are derived from the log-probabilities of the generated tokens.

## Pipeline Overview

1.  **Feature Calculation**: Extract probability-based features (MTP, AvgTP, Mpd, mps) from generation logs.
2.  **Merging**: Combine the calculated features with ground-truth judgments.
3.  **Evaluation**: Train a logistic regression classifier on the features and evaluate performance using bootstrap resampling.

## Scripts

### 1. `compute_prob_features.py`
Calculates uncertainty features for each generated response based on token log-probabilities.

**Features calculated:**
*   `mtp`: Minimum Token Probability.
*   `avgtp`: Average Token Probability.
*   `Mpd`: Maximum Probability Deviation (Probability of top token - Probability of generated token).
*   `mps_approx`: Minimum Probability Spread (Probability of top token - Probability of bottom token in top-k).

**Usage:**
```bash
python compute_prob_features.py \
    "path/to/generation_with_logprobs.json" \
    "outputs/features.json"
```

> **Note**: This script assumes the input JSON contains a `full_info` field with `detailed_logprobs` (as generated by `QA_data_generation.py`).

### 2. `merge_features_proba.py`
Merges the calculated features with `judgment` labels from a separate file.

**Usage:**
```bash
python ../common/merge_features_proba.py \
    "outputs/features.json" \
    "path/to/judged_data.jsonl" \
    "outputs/training_data.json"
```

### 3. `evaluate_features_proba.py`
Evaluates the utility of the features for detecting hallucinations.

**Modes:**
*   **Evaluation (Bootstrap)**: Trains a Logistic Regression model on the data and performs 1000 bootstrap iterations to report Mean ROC-AUC and PR-AUC. Saves the mean model coefficients.
*   **Inference (Pre-trained)**: Evaluates a fixed set of coefficients on a new dataset.

**Usage:**
```bash
# 1. Train and Evaluate (Bootstrap)
python evaluate_features_proba.py \
    "outputs/training_data.json" \
    --output_model_file "outputs/model_params.json"

# 2. Evaluate using pre-trained coefficients
python evaluate_features_proba.py \
    "outputs/new_data.json" \
    --model_coefficients "outputs/model_params.json"
```
