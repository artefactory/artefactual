{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c4d5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.14 environment at: /home/gjeannin/artefactual/.venv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 120ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m artefactual\u001b[2m @ file:///home/gjeannin/artefactual\u001b[0m            \n",
      "\u001b[2K\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 120ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m artefactual\u001b[2m @ file:///home/gjeannin/artefactual\u001b[0m            \n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m artefactual\u001b[2m @ file:///home/gjeannin/artefactual\u001b[0m    \u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m artefactual\u001b[2m @ file:///home/gjeannin/artefactual\u001b[0m    \u001b[1A\n",
      "\u001b[2K\u001b[1A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m artefactual\u001b[2m @ file:///home/gjeannin/artefactual\u001b[0m    \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 393ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m(from file:///home/gjeannin/artefa\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1martefactual\u001b[0m\u001b[2m==0.1.0 (from file:///home/gjeannin/artefactual)\u001b[0m\n",
      "\u001b[2K\u001b[1A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m artefactual\u001b[2m @ file:///home/gjeannin/artefactual\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 393ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m(from file:///home/gjeannin/artefa\u001b[0m\n",
      " \u001b[33m~\u001b[39m \u001b[1martefactual\u001b[0m\u001b[2m==0.1.0 (from file:///home/gjeannin/artefactual)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the package in editable mode\n",
    "!uv pip install -e .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc08ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from artefactual.scoring import EPR, WEPR\n",
    "\n",
    "project_root = pathlib.Path.cwd().parent.resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726c437",
   "metadata": {},
   "source": [
    "## 1. Generate Response with vLLM\n",
    "\n",
    "We use vLLM to generate a response for a single question.\n",
    "We request logprobs to compute entropy-based scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34488b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-24 13:22:57 [utils.py:253] non-default args: {'seed': None, 'disable_log_stats': True, 'model': 'mistralai/Ministral-8B-Instruct-2410'}\n",
      "INFO 12-24 13:22:58 [model.py:637] Resolved architecture: MistralForCausalLM\n",
      "INFO 12-24 13:22:58 [model.py:1750] Using max model len 131072\n",
      "INFO 12-24 13:22:58 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 12-24 13:22:58 [model.py:637] Resolved architecture: MistralForCausalLM\n",
      "INFO 12-24 13:22:58 [model.py:1750] Using max model len 131072\n",
      "INFO 12-24 13:22:58 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-12-24 13:22:58] INFO tekken.py:176: Adding special tokens <SPECIAL_20>, ..., <SPECIAL_999>\n",
      "[2025-12-24 13:22:58] INFO tekken.py:540: Vocab size: 150000\n",
      "[2025-12-24 13:22:58] INFO tekken.py:544: Cutting vocab to first 130072 tokens.\n",
      "[2025-12-24 13:22:58] INFO tekken.py:540: Vocab size: 150000\n",
      "[2025-12-24 13:22:58] INFO tekken.py:544: Cutting vocab to first 130072 tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:00 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='mistralai/Ministral-8B-Instruct-2410', speculative_config=None, tokenizer='mistralai/Ministral-8B-Instruct-2410', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=mistralai/Ministral-8B-Instruct-2410, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}\n",
      "INFO 12-24 13:23:00 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='mistralai/Ministral-8B-Instruct-2410', speculative_config=None, tokenizer='mistralai/Ministral-8B-Instruct-2410', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=mistralai/Ministral-8B-Instruct-2410, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:00 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.100.0.201:32913 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:00 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.100.0.201:32913 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:00 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:00 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:01 [gpu_model_runner.py:3467] Starting to load model mistralai/Ministral-8B-Instruct-2410...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:01 [gpu_model_runner.py:3467] Starting to load model mistralai/Ministral-8B-Instruct-2410...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:02 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:02 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:02 [weight_utils.py:527] No consolidated.safetensors.index.json found in remote.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:02 [weight_utils.py:527] No consolidated.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.24s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.24s/it]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m \n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.24s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.24s/it]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:06 [default_loader.py:308] Loading weights took 3.45 seconds\n",
      "INFO 12-24 13:23:06 [default_loader.py:308] Loading weights took 3.45 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:06 [gpu_model_runner.py:3549] Model loading took 14.9694 GiB memory and 4.328569 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:06 [gpu_model_runner.py:3549] Model loading took 14.9694 GiB memory and 4.328569 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:11 [backends.py:655] Using cache directory: /home/gjeannin/.cache/vllm/torch_compile_cache/905ce91c15/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:11 [backends.py:715] Dynamo bytecode transform time: 4.07 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:11 [backends.py:655] Using cache directory: /home/gjeannin/.cache/vllm/torch_compile_cache/905ce91c15/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:11 [backends.py:715] Dynamo bytecode transform time: 4.07 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:13 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.606 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:13 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.606 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:14 [monitor.py:34] torch.compile takes 5.67 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:14 [monitor.py:34] torch.compile takes 5.67 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:15 [gpu_worker.py:359] Available KV cache memory: 26.38 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:15 [gpu_worker.py:359] Available KV cache memory: 26.38 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:16 [kv_cache_utils.py:1286] GPU KV cache size: 192,064 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:16 [kv_cache_utils.py:1291] Maximum concurrency for 131,072 tokens per request: 3.02x\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:16 [kv_cache_utils.py:1286] GPU KV cache size: 192,064 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:16 [kv_cache_utils.py:1291] Maximum concurrency for 131,072 tokens per request: 3.02x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 23.31it/s]\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 23.31it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 28.67it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 28.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:20 [gpu_model_runner.py:4466] Graph capturing finished in 4 secs, took 0.59 GiB\n",
      "INFO 12-24 13:23:20 [gpu_model_runner.py:4466] Graph capturing finished in 4 secs, took 0.59 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:20 [core.py:254] init engine (profile, create kv cache, warmup model) took 13.77 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m INFO 12-24 13:23:20 [core.py:254] init engine (profile, create kv cache, warmup model) took 13.77 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m [2025-12-24 13:23:20] INFO tekken.py:176: Adding special tokens <SPECIAL_20>, ..., <SPECIAL_999>\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m [2025-12-24 13:23:20] INFO tekken.py:540: Vocab size: 150000\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m [2025-12-24 13:23:20] INFO tekken.py:544: Cutting vocab to first 130072 tokens.\n",
      "[2025-12-24 13:23:20] INFO tekken.py:176: Adding special tokens <SPECIAL_20>, ..., <SPECIAL_999>\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m [2025-12-24 13:23:20] INFO tekken.py:540: Vocab size: 150000\n",
      "\u001b[0;36m(EngineCore_DP0 pid=4023822)\u001b[0;0m [2025-12-24 13:23:20] INFO tekken.py:544: Cutting vocab to first 130072 tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-24 13:23:22 [llm.py:343] Supported tasks: ['generate']\n",
      "Generating response...\n",
      "Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 316.46it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.24s/it, est. speed input: 1.21 toks/s, output: 3.64 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:  The current president of the United States is Joseph R. Biden Jr. He took office on January 20, 2021.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize vLLM\n",
    "# Ensure you have access to the model or change the model name to one you have access to\n",
    "model_name = \"mistralai/Ministral-8B-Instruct-2410\"\n",
    "\n",
    "# Set CUDA_VISIBLE_DEVICES to use a free GPU (e.g., GPU 2)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "llm = LLM(model=model_name)\n",
    "\n",
    "# Define the prompt\n",
    "prompts = [\"Who is the president of the United States?\"]\n",
    "\n",
    "# Define sampling parameters\n",
    "# We need logprobs for WEPR/EPR computation\n",
    "sampling_params = SamplingParams(temperature=0.0, logprobs=20, max_tokens=50)\n",
    "\n",
    "# Generate response\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the generated text\n",
    "generated_text = outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148fe7c",
   "metadata": {},
   "source": [
    "## 2. Initialize WEPR with Custom Weights\n",
    "\n",
    "We initialize the `WEPR` class by pointing it to the specific weights file for the Ministral model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a235291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to the weights file\n",
    "# Assuming the notebook is in 'examples/' and the file is in 'src/artefactual/data/'\n",
    "weights_path = project_root / \"src/artefactual/data/weights_ministral.json\"\n",
    "\n",
    "# Check if file exists\n",
    "if not weights_path.exists():\n",
    "    msg = f\"Weights file not found at: {weights_path}\"\n",
    "    raise FileNotFoundError(msg)\n",
    "\n",
    "# Initialize WEPR\n",
    "# The 'model' argument can be a path to a JSON file containing the weights\n",
    "wepr = WEPR(model=str(weights_path))\n",
    "epr = EPR(model=str(project_root / \"src/artefactual/data/calibration_ministral.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484b26b",
   "metadata": {},
   "source": [
    "## 3. Compute Scores for All Samples\n",
    "\n",
    "We compute WEPR and EPR scores for all 10 samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0480ac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RequestOutput(request_id=0, prompt='Who is the president of the United States?', prompt_token_ids=[1, 31500, 1395, 1278, 10233, 1307, 1278, 4857, 6086, 1063], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' The current president of the United States is Joseph R. Biden Jr. He took office on January 20, 2021.', token_ids=[1531, 3519, 10233, 1307, 1278, 4857, 6086, 1395, 11493, 1434, 1046, 66224, 16791, 1046, 2182, 5244, 9750, 1408, 6866, 1032, 1050, 1048, 1044, 1032, 1050, 1048, 1050, 1049, 1046, 2], cumulative_logprob=-5.926176261856085, logprobs=[{1531: Logprob(logprob=-1.5777908563613892, rank=1, decoded_token=' The'), 14424: Logprob(logprob=-2.4215407371520996, rank=2, decoded_token=' Joe'), 20398: Logprob(logprob=-2.8590407371520996, rank=3, decoded_token=' Who'), 24898: Logprob(logprob=-2.8590407371520996, rank=4, decoded_token=' Donald'), 1319: Logprob(logprob=-3.1402907371520996, rank=5, decoded_token=' ('), 10349: Logprob(logprob=-3.4527907371520996, rank=6, decoded_token=' President'), 5675: Logprob(logprob=-3.6090407371520996, rank=7, decoded_token=' What'), 1362: Logprob(logprob=-3.9215407371520996, rank=8, decoded_token=' I'), 11493: Logprob(logprob=-4.1402907371521, rank=9, decoded_token=' Joseph'), 3060: Logprob(logprob=-4.4527907371521, rank=10, decoded_token=' And'), 1656: Logprob(logprob=-4.4527907371521, rank=11, decoded_token=' In'), 1032: Logprob(logprob=-4.4840407371521, rank=12, decoded_token=' '), 2: Logprob(logprob=-4.5152907371521, rank=13, decoded_token='</s>'), 9380: Logprob(logprob=-4.6402907371521, rank=14, decoded_token=' Here'), 2409: Logprob(logprob=-4.6715407371521, rank=15, decoded_token=' This'), 2157: Logprob(logprob=-4.7027907371521, rank=16, decoded_token=' It'), 3316: Logprob(logprob=-4.7027907371521, rank=17, decoded_token=' Is'), 3075: Logprob(logprob=-4.7340407371521, rank=18, decoded_token=' How'), 68779: Logprob(logprob=-4.7652907371521, rank=19, decoded_token=' Barack'), 2182: Logprob(logprob=-4.8277907371521, rank=20, decoded_token=' He')}, {3519: Logprob(logprob=-0.7039960026741028, rank=1, decoded_token=' current'), 10233: Logprob(logprob=-1.578995943069458, rank=2, decoded_token=' president'), 10349: Logprob(logprob=-1.891495943069458, rank=3, decoded_token=' President'), 1032: Logprob(logprob=-3.703995943069458, rank=4, decoded_token=' '), 4857: Logprob(logprob=-3.891495943069458, rank=5, decoded_token=' United'), 4832: Logprob(logprob=-4.703996181488037, rank=6, decoded_token=' answer'), 2965: Logprob(logprob=-4.891496181488037, rank=7, decoded_token=' person'), 4098: Logprob(logprob=-4.953996181488037, rank=8, decoded_token=' question'), 1608: Logprob(logprob=-5.391496181488037, rank=9, decoded_token=' U'), 2988: Logprob(logprob=-5.703996181488037, rank=10, decoded_token=' present'), 4798: Logprob(logprob=-5.766496181488037, rank=11, decoded_token=' title'), 2564: Logprob(logprob=-5.828996181488037, rank=12, decoded_token=' name'), 3804: Logprob(logprob=-5.828996181488037, rank=13, decoded_token=' last'), 2725: Logprob(logprob=-5.891496181488037, rank=14, decoded_token=' most'), 3629: Logprob(logprob=-6.016496181488037, rank=15, decoded_token=' following'), 61486: Logprob(logprob=-6.203996181488037, rank=16, decoded_token=' incumbent'), 18217: Logprob(logprob=-6.328996181488037, rank=17, decoded_token=' latest'), 3980: Logprob(logprob=-6.453996181488037, rank=18, decoded_token=' position'), 4934: Logprob(logprob=-6.516496181488037, rank=19, decoded_token=' role'), 9750: Logprob(logprob=-6.547746181488037, rank=20, decoded_token=' office')}, {10233: Logprob(logprob=-0.40860816836357117, rank=1, decoded_token=' president'), 10349: Logprob(logprob=-1.1586081981658936, rank=2, decoded_token=' President'), 1608: Logprob(logprob=-5.3461079597473145, rank=3, decoded_token=' U'), 1321: Logprob(logprob=-5.6586079597473145, rank=4, decoded_token=' and'), 4857: Logprob(logprob=-5.9086079597473145, rank=5, decoded_token=' United'), 1319: Logprob(logprob=-6.5961079597473145, rank=6, decoded_token=' ('), 1390: Logprob(logprob=-7.1586079597473145, rank=7, decoded_token=' P'), 1032: Logprob(logprob=-7.4086079597473145, rank=8, decoded_token=' '), 18970: Logprob(logprob=-7.4711079597473145, rank=9, decoded_token=' sitting'), 3825: Logprob(logprob=-7.6586079597473145, rank=10, decoded_token=' US'), 1044: Logprob(logprob=-8.096108436584473, rank=11, decoded_token=','), 16542: Logprob(logprob=-8.346108436584473, rank=12, decoded_token=' elected'), 61486: Logprob(logprob=-8.408608436584473, rank=13, decoded_token=' incumbent'), 28241: Logprob(logprob=-8.471108436584473, rank=14, decoded_token=' commander'), 13485: Logprob(logprob=-8.471108436584473, rank=15, decoded_token=' leader'), 9750: Logprob(logprob=-8.533608436584473, rank=16, decoded_token=' office'), 16813: Logprob(logprob=-8.596108436584473, rank=17, decoded_token=' vice'), 1395: Logprob(logprob=-8.658608436584473, rank=18, decoded_token=' is'), 5150: Logprob(logprob=-8.658608436584473, rank=19, decoded_token=' American'), 21892: Logprob(logprob=-8.658608436584473, rank=20, decoded_token=' serving')}, {1307: Logprob(logprob=-0.23126402497291565, rank=1, decoded_token=' of'), 1395: Logprob(logprob=-1.6062639951705933, rank=2, decoded_token=' is'), 1044: Logprob(logprob=-5.731264114379883, rank=3, decoded_token=','), 1435: Logprob(logprob=-7.793764114379883, rank=4, decoded_token=' as'), 1321: Logprob(logprob=-8.231264114379883, rank=5, decoded_token=' and'), 1681: Logprob(logprob=-8.418764114379883, rank=6, decoded_token=\"'s\"), 1294: Logprob(logprob=-8.543764114379883, rank=7, decoded_token=' in'), 14424: Logprob(logprob=-8.731264114379883, rank=8, decoded_token=' Joe'), 66949: Logprob(logprob=-8.793764114379883, rank=9, decoded_token='-elect'), 1319: Logprob(logprob=-9.293764114379883, rank=10, decoded_token=' ('), 4136: Logprob(logprob=-9.356264114379883, rank=11, decoded_token=' since'), 1486: Logprob(logprob=-9.543764114379883, rank=12, decoded_token=' was'), 1934: Logprob(logprob=-9.668764114379883, rank=13, decoded_token=' has'), 16542: Logprob(logprob=-9.918764114379883, rank=14, decoded_token=' elected'), 1267: Logprob(logprob=-10.043764114379883, rank=15, decoded_token='\\n\\n'), 2190: Logprob(logprob=-10.106264114379883, rank=16, decoded_token='’s'), 24898: Logprob(logprob=-10.418764114379883, rank=17, decoded_token=' Donald'), 1710: Logprob(logprob=-10.543764114379883, rank=18, decoded_token=' can'), 23568: Logprob(logprob=-10.606264114379883, rank=19, decoded_token=' serves'), 1063: Logprob(logprob=-10.606264114379883, rank=20, decoded_token='?')}, {1278: Logprob(logprob=-0.00037079135654494166, rank=1, decoded_token=' the'), 4857: Logprob(logprob=-8.625370979309082, rank=2, decoded_token=' United'), 1531: Logprob(logprob=-10.062870979309082, rank=3, decoded_token=' The'), 9386: Logprob(logprob=-11.062870979309082, rank=4, decoded_token=' America'), 1032: Logprob(logprob=-11.437870979309082, rank=5, decoded_token=' '), 1267: Logprob(logprob=-11.625370979309082, rank=6, decoded_token='\\n\\n'), 1395: Logprob(logprob=-11.687870979309082, rank=7, decoded_token=' is'), 1010: Logprob(logprob=-11.937870979309082, rank=8, decoded_token='\\n'), 2948: Logprob(logprob=-12.000370979309082, rank=9, decoded_token=' our'), 1593: Logprob(logprob=-12.125370979309082, rank=10, decoded_token=' this'), 2: Logprob(logprob=-12.187870979309082, rank=11, decoded_token='</s>'), 3265: Logprob(logprob=-12.437870979309082, rank=12, decoded_token='the'), 8152: Logprob(logprob=-12.687870979309082, rank=13, decoded_token=' USA'), 2576: Logprob(logprob=-13.000370979309082, rank=14, decoded_token=' these'), 9283: Logprob(logprob=-13.000370979309082, rank=15, decoded_token='...\\n\\n'), 10260: Logprob(logprob=-13.250370979309082, rank=16, decoded_token='…\\n\\n'), 1514: Logprob(logprob=-13.312870979309082, rank=17, decoded_token=' he'), 16822: Logprob(logprob=-13.500370979309082, rank=18, decoded_token=' Mexico'), 2143: Logprob(logprob=-13.562870979309082, rank=19, decoded_token=' your'), 1326: Logprob(logprob=-13.562870979309082, rank=20, decoded_token=' th')}, {4857: Logprob(logprob=-0.0007925468380562961, rank=1, decoded_token=' United'), 57969: Logprob(logprob=-8.688292503356934, rank=2, decoded_token=' united'), 8152: Logprob(logprob=-8.688292503356934, rank=3, decoded_token=' USA'), 1608: Logprob(logprob=-8.750792503356934, rank=4, decoded_token=' U'), 35545: Logprob(logprob=-9.125792503356934, rank=5, decoded_token='United'), 3825: Logprob(logprob=-10.188292503356934, rank=6, decoded_token=' US'), 2159: Logprob(logprob=-10.250792503356934, rank=7, decoded_token=' Un'), 64838: Logprob(logprob=-11.438292503356934, rank=8, decoded_token=' Unt'), 1395: Logprob(logprob=-11.563292503356934, rank=9, decoded_token=' is'), 1267: Logprob(logprob=-11.688292503356934, rank=10, decoded_token='\\n\\n'), 6816: Logprob(logprob=-11.875792503356934, rank=11, decoded_token=' country'), 1032: Logprob(logprob=-11.938292503356934, rank=12, decoded_token=' '), 1603: Logprob(logprob=-11.938292503356934, rank=13, decoded_token=' **'), 1010: Logprob(logprob=-12.313292503356934, rank=14, decoded_token='\\n'), 98512: Logprob(logprob=-12.563292503356934, rank=15, decoded_token=' Unite'), 9283: Logprob(logprob=-13.000792503356934, rank=16, decoded_token='...\\n\\n'), 1278: Logprob(logprob=-13.250792503356934, rank=17, decoded_token=' the'), 9557: Logprob(logprob=-13.250792503356934, rank=18, decoded_token=' Republic'), 10260: Logprob(logprob=-13.313292503356934, rank=19, decoded_token='…\\n\\n'), 2: Logprob(logprob=-13.438292503356934, rank=20, decoded_token='</s>')}, {6086: Logprob(logprob=-0.00047291061491705477, rank=1, decoded_token=' States'), 6496: Logprob(logprob=-9.312973022460938, rank=2, decoded_token=' State'), 1435: Logprob(logprob=-9.312973022460938, rank=3, decoded_token=' as'), 52653: Logprob(logprob=-9.812973022460938, rank=4, decoded_token='States'), 1335: Logprob(logprob=-10.000473022460938, rank=5, decoded_token=' S'), 8164: Logprob(logprob=-10.250473022460938, rank=6, decoded_token=' states'), 1267: Logprob(logprob=-10.750473022460938, rank=7, decoded_token='\\n\\n'), 1010: Logprob(logprob=-10.812973022460938, rank=8, decoded_token='\\n'), 1395: Logprob(logprob=-11.250473022460938, rank=9, decoded_token=' is'), 1032: Logprob(logprob=-11.375473022460938, rank=10, decoded_token=' '), 1757: Logprob(logprob=-11.687973022460938, rank=11, decoded_token=' St'), 2: Logprob(logprob=-12.250473022460938, rank=12, decoded_token='</s>'), 21882: Logprob(logprob=-12.312973022460938, rank=13, decoded_token=' Nations'), 16585: Logprob(logprob=-12.625473022460938, rank=14, decoded_token=' Sta'), 9283: Logprob(logprob=-12.687973022460938, rank=15, decoded_token='...\\n\\n'), 11385: Logprob(logprob=-13.000473022460938, rank=16, decoded_token=' Stat'), 1603: Logprob(logprob=-13.062973022460938, rank=17, decoded_token=' **'), 1044: Logprob(logprob=-13.125473022460938, rank=18, decoded_token=','), 1934: Logprob(logprob=-13.125473022460938, rank=19, decoded_token=' has'), 2813: Logprob(logprob=-13.187973022460938, rank=20, decoded_token=' As')}, {1395: Logprob(logprob=-0.018162839114665985, rank=1, decoded_token=' is'), 1307: Logprob(logprob=-4.643162727355957, rank=2, decoded_token=' of'), 1044: Logprob(logprob=-5.393162727355957, rank=3, decoded_token=','), 1435: Logprob(logprob=-6.393162727355957, rank=4, decoded_token=' as'), 1319: Logprob(logprob=-6.768162727355957, rank=5, decoded_token=' ('), 4136: Logprob(logprob=-7.768162727355957, rank=6, decoded_token=' since'), 1486: Logprob(logprob=-9.580662727355957, rank=7, decoded_token=' was'), 14424: Logprob(logprob=-9.643162727355957, rank=8, decoded_token=' Joe'), 1294: Logprob(logprob=-9.955662727355957, rank=9, decoded_token=' in'), 1562: Logprob(logprob=-10.205662727355957, rank=10, decoded_token=' from'), 1934: Logprob(logprob=-10.393162727355957, rank=11, decoded_token=' has'), 1710: Logprob(logprob=-10.580662727355957, rank=12, decoded_token=' can'), 1321: Logprob(logprob=-10.830662727355957, rank=13, decoded_token=' and'), 1394: Logprob(logprob=-10.955662727355957, rank=14, decoded_token=' for'), 1046: Logprob(logprob=-11.205662727355957, rank=15, decoded_token='.'), 23568: Logprob(logprob=-11.205662727355957, rank=16, decoded_token=' serves'), 1513: Logprob(logprob=-11.205662727355957, rank=17, decoded_token=' at'), 1584: Logprob(logprob=-11.393162727355957, rank=18, decoded_token=' are'), 16542: Logprob(logprob=-11.518162727355957, rank=19, decoded_token=' elected'), 1063: Logprob(logprob=-11.768162727355957, rank=20, decoded_token='?')}, {11493: Logprob(logprob=-0.9046202898025513, rank=1, decoded_token=' Joseph'), 14424: Logprob(logprob=-1.0296202898025513, rank=2, decoded_token=' Joe'), 1603: Logprob(logprob=-2.0921201705932617, rank=3, decoded_token=' **'), 2100: Logprob(logprob=-2.9671201705932617, rank=4, decoded_token=':\\n\\n'), 10349: Logprob(logprob=-3.4671201705932617, rank=5, decoded_token=' President'), 10133: Logprob(logprob=-5.467120170593262, rank=6, decoded_token=' Mr'), 24898: Logprob(logprob=-5.779620170593262, rank=7, decoded_token=' Donald'), 1877: Logprob(logprob=-5.904620170593262, rank=8, decoded_token=':\\n'), 1267: Logprob(logprob=-6.217120170593262, rank=9, decoded_token='\\n\\n'), 1278: Logprob(logprob=-6.529620170593262, rank=10, decoded_token=' the'), 1766: Logprob(logprob=-6.717120170593262, rank=11, decoded_token=' ['), 1364: Logprob(logprob=-6.717120170593262, rank=12, decoded_token=' *'), 66224: Logprob(logprob=-6.842120170593262, rank=13, decoded_token=' Biden'), 1058: Logprob(logprob=-7.029620170593262, rank=14, decoded_token=':'), 1605: Logprob(logprob=-7.279620170593262, rank=15, decoded_token=' not'), 9283: Logprob(logprob=-7.279620170593262, rank=16, decoded_token='...\\n\\n'), 1957: Logprob(logprob=-7.342120170593262, rank=17, decoded_token=' _'), 68779: Logprob(logprob=-7.779620170593262, rank=18, decoded_token=' Barack'), 1507: Logprob(logprob=-8.092120170593262, rank=19, decoded_token=' J'), 2: Logprob(logprob=-8.092120170593262, rank=20, decoded_token='</s>')}, {1434: Logprob(logprob=-0.053191956132650375, rank=1, decoded_token=' R'), 29096: Logprob(logprob=-3.740691900253296, rank=2, decoded_token=' Robin'), 66224: Logprob(logprob=-3.803191900253296, rank=3, decoded_token=' Biden'), 1319: Logprob(logprob=-6.678192138671875, rank=4, decoded_token=' ('), 1429: Logprob(logprob=-6.928192138671875, rank=5, decoded_token=' \"'), 8052: Logprob(logprob=-7.178192138671875, rank=6, decoded_token=' Robert'), 11493: Logprob(logprob=-8.553192138671875, rank=7, decoded_token=' Joseph'), 2129: Logprob(logprob=-8.928192138671875, rank=8, decoded_token=' “'), 1398: Logprob(logprob=-8.990692138671875, rank=9, decoded_token=' B'), 9225: Logprob(logprob=-9.365692138671875, rank=10, decoded_token=' Rob'), 101068: Logprob(logprob=-9.490692138671875, rank=11, decoded_token='Robin'), 30870: Logprob(logprob=-9.615692138671875, rank=12, decoded_token=' Raymond'), 29301: Logprob(logprob=-9.990692138671875, rank=13, decoded_token=' Robinson'), 1390: Logprob(logprob=-9.990692138671875, rank=14, decoded_token=' P'), 14424: Logprob(logprob=-10.115692138671875, rank=15, decoded_token=' Joe'), 1439: Logprob(logprob=-10.365692138671875, rank=16, decoded_token=' F'), 1349: Logprob(logprob=-10.428192138671875, rank=17, decoded_token=' A'), 7504: Logprob(logprob=-10.428192138671875, rank=18, decoded_token=' (\"'), 19420: Logprob(logprob=-10.490692138671875, rank=19, decoded_token=' Patrick'), 1507: Logprob(logprob=-10.740692138671875, rank=20, decoded_token=' J')}, {1046: Logprob(logprob=-0.0021115881390869617, rank=1, decoded_token='.'), 66224: Logprob(logprob=-6.564611434936523, rank=2, decoded_token=' Biden'), 1338: Logprob(logprob=-8.439611434936523, rank=3, decoded_token='.\\n\\n'), 7138: Logprob(logprob=-9.627111434936523, rank=4, decoded_token='.B'), 6286: Logprob(logprob=-9.689611434936523, rank=5, decoded_token='iden'), 1626: Logprob(logprob=-10.502111434936523, rank=6, decoded_token='.\\n'), 10083: Logprob(logprob=-11.127111434936523, rank=7, decoded_token='undle'), 3109: Logprob(logprob=-11.877111434936523, rank=8, decoded_token='.,'), 118102: Logprob(logprob=-12.314611434936523, rank=9, decoded_token='inaldo'), 3025: Logprob(logprob=-12.564611434936523, rank=10, decoded_token='omb'), 2247: Logprob(logprob=-12.564611434936523, rank=11, decoded_token=' .'), 6291: Logprob(logprob=-12.627111434936523, rank=12, decoded_token='atz'), 26667: Logprob(logprob=-12.752111434936523, rank=13, decoded_token='.('), 6082: Logprob(logprob=-12.752111434936523, rank=14, decoded_token='iddle'), 1791: Logprob(logprob=-12.877111434936523, rank=15, decoded_token='..'), 3031: Logprob(logprob=-12.877111434936523, rank=16, decoded_token='ino'), 1912: Logprob(logprob=-12.877111434936523, rank=17, decoded_token='inal'), 7659: Logprob(logprob=-12.939611434936523, rank=18, decoded_token='idden'), 4910: Logprob(logprob=-13.002111434936523, rank=19, decoded_token='idd'), 5791: Logprob(logprob=-13.064611434936523, rank=20, decoded_token='iding')}, {66224: Logprob(logprob=-0.001387705677188933, rank=1, decoded_token=' Biden'), 11493: Logprob(logprob=-7.813887596130371, rank=2, decoded_token=' Joseph'), 14424: Logprob(logprob=-8.376387596130371, rank=3, decoded_token=' Joe'), 1429: Logprob(logprob=-8.626387596130371, rank=4, decoded_token=' \"'), 2129: Logprob(logprob=-10.251387596130371, rank=5, decoded_token=' “'), 6194: Logprob(logprob=-10.438887596130371, rank=6, decoded_token=' node'), 84159: Logprob(logprob=-10.595137596130371, rank=7, decoded_token=' Bid'), 15893: Logprob(logprob=-10.657637596130371, rank=8, decoded_token=' Node'), 68779: Logprob(logprob=-10.720137596130371, rank=9, decoded_token=' Barack'), 1398: Logprob(logprob=-10.813887596130371, rank=10, decoded_token=' B'), 31261: Logprob(logprob=-11.063887596130371, rank=11, decoded_token=' Baker'), 34743: Logprob(logprob=-11.126387596130371, rank=12, decoded_token=' Obama'), 1319: Logprob(logprob=-11.188887596130371, rank=13, decoded_token=' ('), 28139: Logprob(logprob=-11.313887596130371, rank=14, decoded_token=' Barn'), 86241: Logprob(logprob=-11.407637596130371, rank=15, decoded_token=' Bing'), 27640: Logprob(logprob=-11.595137596130371, rank=16, decoded_token=' Bush'), 9822: Logprob(logprob=-11.595137596130371, rank=17, decoded_token='node'), 24898: Logprob(logprob=-11.626387596130371, rank=18, decoded_token=' Donald'), 7504: Logprob(logprob=-12.001387596130371, rank=19, decoded_token=' (\"'), 95226: Logprob(logprob=-12.126387596130371, rank=20, decoded_token='拜')}, {16791: Logprob(logprob=-0.020270103588700294, rank=1, decoded_token=' Jr'), 1044: Logprob(logprob=-4.145269870758057, rank=2, decoded_token=','), 1046: Logprob(logprob=-5.770269870758057, rank=3, decoded_token='.'), 1338: Logprob(logprob=-7.270269870758057, rank=4, decoded_token='.\\n\\n'), 26979: Logprob(logprob=-8.395270347595215, rank=5, decoded_token=' Junior'), 1319: Logprob(logprob=-10.020270347595215, rank=6, decoded_token=' ('), 35306: Logprob(logprob=-10.645270347595215, rank=7, decoded_token=' junior'), 83173: Logprob(logprob=-10.832770347595215, rank=8, decoded_token=' JR'), 2: Logprob(logprob=-10.895270347595215, rank=9, decoded_token='</s>'), 29605: Logprob(logprob=-11.207770347595215, rank=10, decoded_token=' Sr'), 1626: Logprob(logprob=-11.645270347595215, rank=11, decoded_token='.\\n'), 6738: Logprob(logprob=-11.832770347595215, rank=12, decoded_token=' III'), 2274: Logprob(logprob=-12.332770347595215, rank=13, decoded_token=' who'), 1435: Logprob(logprob=-12.520270347595215, rank=14, decoded_token=' as'), 1267: Logprob(logprob=-12.582770347595215, rank=15, decoded_token='\\n\\n'), 14424: Logprob(logprob=-12.645270347595215, rank=16, decoded_token=' Joe'), 4136: Logprob(logprob=-12.645270347595215, rank=17, decoded_token=' since'), 1321: Logprob(logprob=-13.020270347595215, rank=18, decoded_token=' and'), 1507: Logprob(logprob=-13.082770347595215, rank=19, decoded_token=' J'), 123126: Logprob(logprob=-13.145270347595215, rank=20, decoded_token='Junior')}, {1046: Logprob(logprob=-0.1732126772403717, rank=1, decoded_token='.'), 3109: Logprob(logprob=-2.298212766647339, rank=2, decoded_token='.,'), 1338: Logprob(logprob=-2.923212766647339, rank=3, decoded_token='.\\n\\n'), 1791: Logprob(logprob=-6.17321252822876, rank=4, decoded_token='..'), 1044: Logprob(logprob=-6.79821252822876, rank=5, decoded_token=','), 1626: Logprob(logprob=-6.92321252822876, rank=6, decoded_token='.\\n'), 18595: Logprob(logprob=-8.423213005065918, rank=7, decoded_token='..\\n\\n'), 2: Logprob(logprob=-9.173213005065918, rank=8, decoded_token='</s>'), 15132: Logprob(logprob=-9.673213005065918, rank=9, decoded_token='.;'), 1319: Logprob(logprob=-10.423213005065918, rank=10, decoded_token=' ('), 26667: Logprob(logprob=-11.423213005065918, rank=11, decoded_token='.('), 5384: Logprob(logprob=-11.423213005065918, rank=12, decoded_token='./'), 14617: Logprob(logprob=-11.423213005065918, rank=13, decoded_token='.).'), 54871: Logprob(logprob=-11.548213005065918, rank=14, decoded_token=',.'), 1267: Logprob(logprob=-11.548213005065918, rank=15, decoded_token='\\n\\n'), 39653: Logprob(logprob=-11.673213005065918, rank=16, decoded_token='..\\n'), 8925: Logprob(logprob=-11.735713005065918, rank=17, decoded_token='.['), 14223: Logprob(logprob=-11.860713005065918, rank=18, decoded_token='.:'), 2247: Logprob(logprob=-11.985713005065918, rank=19, decoded_token=' .'), 2274: Logprob(logprob=-12.048213005065918, rank=20, decoded_token=' who')}, {2182: Logprob(logprob=-0.06596248596906662, rank=1, decoded_token=' He'), 2: Logprob(logprob=-3.690962553024292, rank=2, decoded_token='</s>'), 9380: Logprob(logprob=-4.190962314605713, rank=3, decoded_token=' Here'), 1319: Logprob(logprob=-5.440962314605713, rank=4, decoded_token=' ('), 6304: Logprob(logprob=-5.753462314605713, rank=5, decoded_token=' His'), 2274: Logprob(logprob=-6.003462314605713, rank=6, decoded_token=' who'), 9748: Logprob(logprob=-6.503462314605713, rank=7, decoded_token=' Since'), 2813: Logprob(logprob=-6.565962314605713, rank=8, decoded_token=' As'), 14424: Logprob(logprob=-6.878462314605713, rank=9, decoded_token=' Joe'), 66224: Logprob(logprob=-6.940962314605713, rank=10, decoded_token=' Biden'), 5417: Logprob(logprob=-7.190962314605713, rank=11, decoded_token=' After'), 1531: Logprob(logprob=-7.378462314605713, rank=12, decoded_token=' The'), 1435: Logprob(logprob=-7.503462314605713, rank=13, decoded_token=' as'), 3886: Logprob(logprob=-7.503462314605713, rank=14, decoded_token=' However'), 1656: Logprob(logprob=-7.565962314605713, rank=15, decoded_token=' In'), 30259: Logprob(logprob=-7.565962314605713, rank=16, decoded_token=' Prior'), 10349: Logprob(logprob=-8.128462791442871, rank=17, decoded_token=' President'), 3452: Logprob(logprob=-8.253462791442871, rank=18, decoded_token='He'), 20398: Logprob(logprob=-8.378462791442871, rank=19, decoded_token=' Who'), 20218: Logprob(logprob=-8.440962791442871, rank=20, decoded_token=' Before')}, {5244: Logprob(logprob=-0.6740008592605591, rank=1, decoded_token=' took'), 16088: Logprob(logprob=-1.049000859260559, rank=2, decoded_token=' assumed'), 1486: Logprob(logprob=-2.4240007400512695, rank=3, decoded_token=' was'), 1934: Logprob(logprob=-3.9240007400512695, rank=4, decoded_token=' has'), 1395: Logprob(logprob=-4.5490007400512695, rank=5, decoded_token=' is'), 5872: Logprob(logprob=-4.5490007400512695, rank=6, decoded_token=' became'), 31109: Logprob(logprob=-5.7990007400512695, rank=7, decoded_token=' succeeded'), 6609: Logprob(logprob=-5.9240007400512695, rank=8, decoded_token=' began'), 12298: Logprob(logprob=-6.4240007400512695, rank=9, decoded_token=' served'), 41633: Logprob(logprob=-7.5490007400512695, rank=10, decoded_token=' assumes'), 23568: Logprob(logprob=-7.7365007400512695, rank=11, decoded_token=' serves'), 4552: Logprob(logprob=-8.04900074005127, rank=12, decoded_token=' won'), 6990: Logprob(logprob=-8.17400074005127, rank=13, decoded_token=' started'), 12217: Logprob(logprob=-8.48650074005127, rank=14, decoded_token=' replaced'), 26295: Logprob(logprob=-8.79900074005127, rank=15, decoded_token=' officially'), 6452: Logprob(logprob=-8.92400074005127, rank=16, decoded_token=' held'), 69584: Logprob(logprob=-9.11150074005127, rank=17, decoded_token='took'), 14063: Logprob(logprob=-9.29900074005127, rank=18, decoded_token=' entered'), 5740: Logprob(logprob=-9.36150074005127, rank=19, decoded_token=' came'), 18751: Logprob(logprob=-9.61150074005127, rank=20, decoded_token=' holds')}, {9750: Logprob(logprob=-0.0028017812874168158, rank=1, decoded_token=' office'), 1278: Logprob(logprob=-6.002801895141602, rank=2, decoded_token=' the'), 70953: Logprob(logprob=-8.502801895141602, rank=3, decoded_token=' oath'), 2136: Logprob(logprob=-10.002801895141602, rank=4, decoded_token=' over'), 92648: Logprob(logprob=-10.252801895141602, rank=5, decoded_token='office'), 1794: Logprob(logprob=-10.940301895141602, rank=6, decoded_token=' his'), 2015: Logprob(logprob=-12.190301895141602, rank=7, decoded_token=' up'), 16711: Logprob(logprob=-12.190301895141602, rank=8, decoded_token=' Office'), 30400: Logprob(logprob=-12.815301895141602, rank=9, decoded_token=' offices'), 10162: Logprob(logprob=-13.440301895141602, rank=10, decoded_token=' charge'), 1408: Logprob(logprob=-13.565301895141602, rank=11, decoded_token=' on'), 100074: Logprob(logprob=-13.815301895141602, rank=12, decoded_token='-office'), 1420: Logprob(logprob=-14.190301895141602, rank=13, decoded_token=' an'), 86039: Logprob(logprob=-14.377801895141602, rank=14, decoded_token=' sworn'), 2490: Logprob(logprob=-14.627801895141602, rank=15, decoded_token=' off'), 1593: Logprob(logprob=-15.002801895141602, rank=16, decoded_token=' this'), 104224: Logprob(logprob=-15.127801895141602, rank=17, decoded_token=' inaugurated'), 1281: Logprob(logprob=-15.127801895141602, rank=18, decoded_token=' o'), 33441: Logprob(logprob=-15.190301895141602, rank=19, decoded_token=' presidential'), 30269: Logprob(logprob=-15.315301895141602, rank=20, decoded_token=' ownership')}, {1408: Logprob(logprob=-0.007876882329583168, rank=1, decoded_token=' on'), 1435: Logprob(logprob=-5.007876873016357, rank=2, decoded_token=' as'), 1294: Logprob(logprob=-7.632876873016357, rank=3, decoded_token=' in'), 2453: Logprob(logprob=-7.757876873016357, rank=4, decoded_token=' after'), 3629: Logprob(logprob=-9.007877349853516, rank=5, decoded_token=' following'), 1513: Logprob(logprob=-10.382877349853516, rank=6, decoded_token=' at'), 6610: Logprob(logprob=-10.632877349853516, rank=7, decoded_token=' upon'), 6866: Logprob(logprob=-10.757877349853516, rank=8, decoded_token=' January'), 3184: Logprob(logprob=-11.382877349853516, rank=9, decoded_token=' during'), 1321: Logprob(logprob=-12.007877349853516, rank=10, decoded_token=' and'), 1562: Logprob(logprob=-12.195377349853516, rank=11, decoded_token=' from'), 1394: Logprob(logprob=-12.695377349853516, rank=12, decoded_token=' for'), 2200: Logprob(logprob=-12.757877349853516, rank=13, decoded_token=' when'), 2425: Logprob(logprob=-12.757877349853516, rank=14, decoded_token=' under'), 26295: Logprob(logprob=-13.257877349853516, rank=15, decoded_token=' officially'), 22594: Logprob(logprob=-13.320377349853516, rank=16, decoded_token=' alongside'), 1454: Logprob(logprob=-13.320377349853516, rank=17, decoded_token=' with'), 1307: Logprob(logprob=-13.570377349853516, rank=18, decoded_token=' of'), 4628: Logprob(logprob=-13.632877349853516, rank=19, decoded_token=' Jan'), 1263: Logprob(logprob=-13.695377349853516, rank=20, decoded_token='on')}, {6866: Logprob(logprob=-0.001254724687896669, rank=1, decoded_token=' January'), 1032: Logprob(logprob=-8.00125503540039, rank=2, decoded_token=' '), 6032: Logprob(logprob=-8.25125503540039, rank=3, decoded_token=' November'), 1656: Logprob(logprob=-8.75125503540039, rank=4, decoded_token=' In'), 60390: Logprob(logprob=-9.00125503540039, rank=5, decoded_token='January'), 1278: Logprob(logprob=-9.00125503540039, rank=6, decoded_token=' the'), 4628: Logprob(logprob=-9.81375503540039, rank=7, decoded_token=' Jan'), 42308: Logprob(logprob=-10.25125503540039, rank=8, decoded_token=' Wednesday'), 14424: Logprob(logprob=-10.62625503540039, rank=9, decoded_token=' Joe'), 7859: Logprob(logprob=-10.62625503540039, rank=10, decoded_token=' February'), 7199: Logprob(logprob=-10.62625503540039, rank=11, decoded_token=' December'), 6653: Logprob(logprob=-11.00125503540039, rank=12, decoded_token=' October'), 120770: Logprob(logprob=-11.81375503540039, rank=13, decoded_token=' inauguration'), 51405: Logprob(logprob=-12.00125503540039, rank=14, decoded_token=' Jana'), 44692: Logprob(logprob=-12.31375503540039, rank=15, decoded_token=' Tuesday'), 18408: Logprob(logprob=-12.31375503540039, rank=16, decoded_token=' Januar'), 43173: Logprob(logprob=-12.31375503540039, rank=17, decoded_token=' Thursday'), 5821: Logprob(logprob=-12.50125503540039, rank=18, decoded_token=' April'), 6776: Logprob(logprob=-12.56375503540039, rank=19, decoded_token=' July'), 1794: Logprob(logprob=-12.81375503540039, rank=20, decoded_token=' his')}, {1032: Logprob(logprob=-1.823885577323381e-05, rank=1, decoded_token=' '), 1044: Logprob(logprob=-11.812518119812012, rank=2, decoded_token=','), 2: Logprob(logprob=-12.375018119812012, rank=3, decoded_token='</s>'), 1046: Logprob(logprob=-14.250018119812012, rank=4, decoded_token='.'), 9283: Logprob(logprob=-14.312518119812012, rank=5, decoded_token='...\\n\\n'), 1267: Logprob(logprob=-14.562518119812012, rank=6, decoded_token='\\n\\n'), 1278: Logprob(logprob=-14.750018119812012, rank=7, decoded_token=' the'), 1603: Logprob(logprob=-14.937518119812012, rank=8, decoded_token=' **'), 10260: Logprob(logprob=-15.125018119812012, rank=9, decoded_token='…\\n\\n'), 1510: Logprob(logprob=-15.125018119812012, rank=10, decoded_token='…'), 2880: Logprob(logprob=-15.187518119812012, rank=11, decoded_token='...'), 1050: Logprob(logprob=-15.375018119812012, rank=12, decoded_token='2'), 38659: Logprob(logprob=-15.625018119812012, rank=13, decoded_token=' ...\\n\\n'), 11496: Logprob(logprob=-15.687518119812012, rank=14, decoded_token=' XX'), 6421: Logprob(logprob=-15.875018119812012, rank=15, decoded_token=' ...'), 11180: Logprob(logprob=-15.937518119812012, rank=16, decoded_token=' …'), 1010: Logprob(logprob=-15.937518119812012, rank=17, decoded_token='\\n'), 1307: Logprob(logprob=-16.125019073486328, rank=18, decoded_token=' of'), 3897: Logprob(logprob=-16.125019073486328, rank=19, decoded_token=' ,'), 16832: Logprob(logprob=-16.187519073486328, rank=20, decoded_token='...\\n')}, {1050: Logprob(logprob=-2.372236667724792e-05, rank=1, decoded_token='2'), 1049: Logprob(logprob=-11.12502384185791, rank=2, decoded_token='1'), 1051: Logprob(logprob=-12.62502384185791, rank=3, decoded_token='3'), 1054: Logprob(logprob=-13.00002384185791, rank=4, decoded_token='6'), 1053: Logprob(logprob=-13.62502384185791, rank=5, decoded_token='5'), 1052: Logprob(logprob=-13.75002384185791, rank=6, decoded_token='4'), 1055: Logprob(logprob=-14.50002384185791, rank=7, decoded_token='7'), 1048: Logprob(logprob=-15.50002384185791, rank=8, decoded_token='0'), 1032: Logprob(logprob=-15.50002384185791, rank=9, decoded_token=' '), 1057: Logprob(logprob=-16.062522888183594, rank=10, decoded_token='9'), 1056: Logprob(logprob=-16.437522888183594, rank=11, decoded_token='8'), 28895: Logprob(logprob=-20.250022888183594, rank=12, decoded_token='tw'), 18860: Logprob(logprob=-20.250022888183594, rank=13, decoded_token='XX'), 6837: Logprob(logprob=-20.437522888183594, rank=14, decoded_token='۲'), 60390: Logprob(logprob=-20.562522888183594, rank=15, decoded_token='January'), 51159: Logprob(logprob=-21.312522888183594, rank=16, decoded_token=' twentieth'), 16077: Logprob(logprob=-21.500022888183594, rank=17, decoded_token='²'), 4536: Logprob(logprob=-21.562522888183594, rank=18, decoded_token='zo'), 11395: Logprob(logprob=-21.562522888183594, rank=19, decoded_token='iced'), 50881: Logprob(logprob=-21.937522888183594, rank=20, decoded_token='၂')}, {1048: Logprob(logprob=-0.0001567479339428246, rank=1, decoded_token='0'), 1049: Logprob(logprob=-8.87515640258789, rank=2, decoded_token='1'), 1050: Logprob(logprob=-12.00015640258789, rank=3, decoded_token='2'), 1051: Logprob(logprob=-13.00015640258789, rank=4, decoded_token='3'), 1044: Logprob(logprob=-13.12515640258789, rank=5, decoded_token=','), 1057: Logprob(logprob=-13.25015640258789, rank=6, decoded_token='9'), 1055: Logprob(logprob=-13.62515640258789, rank=7, decoded_token='7'), 2279: Logprob(logprob=-13.75015640258789, rank=8, decoded_token='oth'), 1052: Logprob(logprob=-14.12515640258789, rank=9, decoded_token='4'), 1053: Logprob(logprob=-14.37515640258789, rank=10, decoded_token='5'), 1056: Logprob(logprob=-14.62515640258789, rank=11, decoded_token='8'), 1054: Logprob(logprob=-14.75015640258789, rank=12, decoded_token='6'), 1298: Logprob(logprob=-16.25015640258789, rank=13, decoded_token='nd'), 1111: Logprob(logprob=-16.31265640258789, rank=14, decoded_token='o'), 2: Logprob(logprob=-16.81265640258789, rank=15, decoded_token='</s>'), 1079: Logprob(logprob=-18.75015640258789, rank=16, decoded_token='O'), 1032: Logprob(logprob=-19.50015640258789, rank=17, decoded_token=' '), 1267: Logprob(logprob=-20.50015640258789, rank=18, decoded_token='\\n\\n'), 1046: Logprob(logprob=-20.56265640258789, rank=19, decoded_token='.'), 72854: Logprob(logprob=-20.68765640258789, rank=20, decoded_token='OTH')}, {1044: Logprob(logprob=-0.0005570290377363563, rank=1, decoded_token=','), 1411: Logprob(logprob=-7.500556945800781, rank=2, decoded_token='th'), 1050: Logprob(logprob=-13.125556945800781, rank=3, decoded_token='2'), 1046: Logprob(logprob=-14.563056945800781, rank=4, decoded_token='.'), 1952: Logprob(logprob=-14.688056945800781, rank=5, decoded_token='^{'), 2: Logprob(logprob=-15.063056945800781, rank=6, decoded_token='</s>'), 6422: Logprob(logprob=-15.625556945800781, rank=7, decoded_token=',\\n\\n'), 1032: Logprob(logprob=-15.750556945800781, rank=8, decoded_token=' '), 3897: Logprob(logprob=-16.25055694580078, rank=9, decoded_token=' ,'), 1520: Logprob(logprob=-16.43805694580078, rank=10, decoded_token=',\\n'), 1338: Logprob(logprob=-17.00055694580078, rank=11, decoded_token='.\\n\\n'), 1307: Logprob(logprob=-17.37555694580078, rank=12, decoded_token=' of'), 1326: Logprob(logprob=-17.50055694580078, rank=13, decoded_token=' th'), 1294: Logprob(logprob=-17.68805694580078, rank=14, decoded_token=' in'), 1513: Logprob(logprob=-17.68805694580078, rank=15, decoded_token=' at'), 64704: Logprob(logprob=-17.87555694580078, rank=16, decoded_token=',,'), 1267: Logprob(logprob=-18.00055694580078, rank=17, decoded_token='\\n\\n'), 1049: Logprob(logprob=-18.18805694580078, rank=18, decoded_token='1'), 1048: Logprob(logprob=-18.25055694580078, rank=19, decoded_token='0'), 1435: Logprob(logprob=-18.37555694580078, rank=20, decoded_token=' as')}, {1032: Logprob(logprob=-1.597391747054644e-05, rank=1, decoded_token=' '), 1050: Logprob(logprob=-11.750016212463379, rank=2, decoded_token='2'), 2: Logprob(logprob=-12.062516212463379, rank=3, decoded_token='</s>'), 6866: Logprob(logprob=-15.062516212463379, rank=4, decoded_token=' January'), 1603: Logprob(logprob=-15.250016212463379, rank=5, decoded_token=' **'), 1278: Logprob(logprob=-15.375016212463379, rank=6, decoded_token=' the'), 14424: Logprob(logprob=-15.562516212463379, rank=7, decoded_token=' Joe'), 38659: Logprob(logprob=-15.562516212463379, rank=8, decoded_token=' ...\\n\\n'), 9283: Logprob(logprob=-15.812516212463379, rank=9, decoded_token='...\\n\\n'), 1656: Logprob(logprob=-15.875016212463379, rank=10, decoded_token=' In'), 3629: Logprob(logprob=-16.750015258789062, rank=11, decoded_token=' following'), 3897: Logprob(logprob=-16.875015258789062, rank=12, decoded_token=' ,'), 1321: Logprob(logprob=-17.062515258789062, rank=13, decoded_token=' and'), 1294: Logprob(logprob=-17.125015258789062, rank=14, decoded_token=' in'), 6421: Logprob(logprob=-17.312515258789062, rank=15, decoded_token=' ...'), 2453: Logprob(logprob=-17.500015258789062, rank=16, decoded_token=' after'), 1513: Logprob(logprob=-17.625015258789062, rank=17, decoded_token=' at'), 10260: Logprob(logprob=-17.687515258789062, rank=18, decoded_token='…\\n\\n'), 20213: Logprob(logprob=-17.937515258789062, rank=19, decoded_token=' ...\\n'), 1531: Logprob(logprob=-17.937515258789062, rank=20, decoded_token=' The')}, {1050: Logprob(logprob=-1.9073468138230965e-06, rank=1, decoded_token='2'), 1049: Logprob(logprob=-14.000001907348633, rank=2, decoded_token='1'), 1032: Logprob(logprob=-14.500001907348633, rank=3, decoded_token=' '), 1052: Logprob(logprob=-14.562501907348633, rank=4, decoded_token='4'), 1051: Logprob(logprob=-15.875001907348633, rank=5, decoded_token='3'), 1048: Logprob(logprob=-17.250001907348633, rank=6, decoded_token='0'), 1053: Logprob(logprob=-17.375001907348633, rank=7, decoded_token='5'), 1055: Logprob(logprob=-18.375001907348633, rank=8, decoded_token='7'), 1057: Logprob(logprob=-18.562501907348633, rank=9, decoded_token='9'), 1054: Logprob(logprob=-18.750001907348633, rank=10, decoded_token='6'), 6837: Logprob(logprob=-19.125001907348633, rank=11, decoded_token='۲'), 14424: Logprob(logprob=-19.187501907348633, rank=12, decoded_token=' Joe'), 57654: Logprob(logprob=-19.375001907348633, rank=13, decoded_token='Joe'), 1056: Logprob(logprob=-19.437501907348633, rank=14, decoded_token='8'), 16385: Logprob(logprob=-20.125001907348633, rank=15, decoded_token=' twenty'), 12629: Logprob(logprob=-20.625001907348633, rank=16, decoded_token='২'), 80711: Logprob(logprob=-20.687501907348633, rank=17, decoded_token='₂'), 66224: Logprob(logprob=-21.000001907348633, rank=18, decoded_token=' Biden'), 31609: Logprob(logprob=-21.062501907348633, rank=19, decoded_token='Donald'), 22734: Logprob(logprob=-21.125001907348633, rank=20, decoded_token='२')}, {1048: Logprob(logprob=-1.4305104514278355e-06, rank=1, decoded_token='0'), 1049: Logprob(logprob=-13.750000953674316, rank=2, decoded_token='1'), 1050: Logprob(logprob=-15.250000953674316, rank=3, decoded_token='2'), 2: Logprob(logprob=-16.062501907348633, rank=4, decoded_token='</s>'), 1051: Logprob(logprob=-16.875001907348633, rank=5, decoded_token='3'), 1057: Logprob(logprob=-17.250001907348633, rank=6, decoded_token='9'), 1052: Logprob(logprob=-17.687501907348633, rank=7, decoded_token='4'), 1053: Logprob(logprob=-17.875001907348633, rank=8, decoded_token='5'), 1111: Logprob(logprob=-18.812501907348633, rank=9, decoded_token='o'), 1056: Logprob(logprob=-18.812501907348633, rank=10, decoded_token='8'), 1054: Logprob(logprob=-19.062501907348633, rank=11, decoded_token='6'), 1055: Logprob(logprob=-19.062501907348633, rank=12, decoded_token='7'), 12078: Logprob(logprob=-19.687501907348633, rank=13, decoded_token='oo'), 1046: Logprob(logprob=-20.000001907348633, rank=14, decoded_token='.'), 1267: Logprob(logprob=-20.000001907348633, rank=15, decoded_token='\\n\\n'), 2880: Logprob(logprob=-20.062501907348633, rank=16, decoded_token='...'), 1298: Logprob(logprob=-20.125001907348633, rank=17, decoded_token='nd'), 9283: Logprob(logprob=-20.187501907348633, rank=18, decoded_token='...\\n\\n'), 1058: Logprob(logprob=-20.437501907348633, rank=19, decoded_token=':'), 1032: Logprob(logprob=-20.437501907348633, rank=20, decoded_token=' ')}, {1050: Logprob(logprob=-2.109982233378105e-05, rank=1, decoded_token='2'), 1049: Logprob(logprob=-11.437520980834961, rank=2, decoded_token='1'), 1048: Logprob(logprob=-12.062520980834961, rank=3, decoded_token='0'), 1044: Logprob(logprob=-12.875020980834961, rank=4, decoded_token=','), 1411: Logprob(logprob=-14.375020980834961, rank=5, decoded_token='th'), 2: Logprob(logprob=-14.750020980834961, rank=6, decoded_token='</s>'), 1046: Logprob(logprob=-15.812520980834961, rank=7, decoded_token='.'), 1051: Logprob(logprob=-16.06252098083496, rank=8, decoded_token='3'), 1032: Logprob(logprob=-16.18752098083496, rank=9, decoded_token=' '), 6866: Logprob(logprob=-16.43752098083496, rank=10, decoded_token=' January'), 1052: Logprob(logprob=-16.81252098083496, rank=11, decoded_token='4'), 16385: Logprob(logprob=-17.18752098083496, rank=12, decoded_token=' twenty'), 1267: Logprob(logprob=-17.18752098083496, rank=13, decoded_token='\\n\\n'), 60390: Logprob(logprob=-17.62502098083496, rank=14, decoded_token='January'), 18860: Logprob(logprob=-17.75002098083496, rank=15, decoded_token='XX'), 1057: Logprob(logprob=-18.37502098083496, rank=16, decoded_token='9'), 1053: Logprob(logprob=-18.37502098083496, rank=17, decoded_token='5'), 1055: Logprob(logprob=-18.56252098083496, rank=18, decoded_token='7'), 1338: Logprob(logprob=-18.68752098083496, rank=19, decoded_token='.\\n\\n'), 1010: Logprob(logprob=-18.84377098083496, rank=20, decoded_token='\\n')}, {1049: Logprob(logprob=-0.00015925093612167984, rank=1, decoded_token='1'), 1048: Logprob(logprob=-8.87515926361084, rank=2, decoded_token='0'), 1050: Logprob(logprob=-11.37515926361084, rank=3, decoded_token='2'), 1051: Logprob(logprob=-12.00015926361084, rank=4, decoded_token='3'), 2: Logprob(logprob=-15.06265926361084, rank=5, decoded_token='</s>'), 1044: Logprob(logprob=-15.06265926361084, rank=6, decoded_token=','), 1057: Logprob(logprob=-15.37515926361084, rank=7, decoded_token='9'), 1053: Logprob(logprob=-15.43765926361084, rank=8, decoded_token='5'), 1046: Logprob(logprob=-15.56265926361084, rank=9, decoded_token='.'), 1052: Logprob(logprob=-15.56265926361084, rank=10, decoded_token='4'), 1267: Logprob(logprob=-16.687658309936523, rank=11, decoded_token='\\n\\n'), 1055: Logprob(logprob=-16.750158309936523, rank=12, decoded_token='7'), 1108: Logprob(logprob=-17.750158309936523, rank=13, decoded_token='l'), 1032: Logprob(logprob=-18.000158309936523, rank=14, decoded_token=' '), 1338: Logprob(logprob=-18.062658309936523, rank=15, decoded_token='.\\n\\n'), 1321: Logprob(logprob=-18.125158309936523, rank=16, decoded_token=' and'), 1054: Logprob(logprob=-18.187658309936523, rank=17, decoded_token='6'), 1656: Logprob(logprob=-18.312658309936523, rank=18, decoded_token=' In'), 1105: Logprob(logprob=-18.937658309936523, rank=19, decoded_token='i'), 1056: Logprob(logprob=-18.937658309936523, rank=20, decoded_token='8')}, {1046: Logprob(logprob=-0.34357666969299316, rank=1, decoded_token='.'), 1044: Logprob(logprob=-1.4685766696929932, rank=2, decoded_token=','), 1338: Logprob(logprob=-2.843576669692993, rank=3, decoded_token='.\\n\\n'), 2453: Logprob(logprob=-6.968576431274414, rank=4, decoded_token=' after'), 3629: Logprob(logprob=-7.718576431274414, rank=5, decoded_token=' following'), 1321: Logprob(logprob=-8.093576431274414, rank=6, decoded_token=' and'), 1626: Logprob(logprob=-8.343576431274414, rank=7, decoded_token='.\\n'), 1435: Logprob(logprob=-8.593576431274414, rank=8, decoded_token=' as'), 2: Logprob(logprob=-9.218576431274414, rank=9, decoded_token='</s>'), 97592: Logprob(logprob=-11.468576431274414, rank=10, decoded_token=' succeeding'), 1059: Logprob(logprob=-12.281076431274414, rank=11, decoded_token=';'), 1267: Logprob(logprob=-12.531076431274414, rank=12, decoded_token='\\n\\n'), 6422: Logprob(logprob=-12.656076431274414, rank=13, decoded_token=',\\n\\n'), 29591: Logprob(logprob=-12.718576431274414, rank=14, decoded_token=' replacing'), 1394: Logprob(logprob=-12.781076431274414, rank=15, decoded_token=' for'), 1317: Logprob(logprob=-12.968576431274414, rank=16, decoded_token=' to'), 1319: Logprob(logprob=-12.968576431274414, rank=17, decoded_token=' ('), 3184: Logprob(logprob=-13.031076431274414, rank=18, decoded_token=' during'), 6610: Logprob(logprob=-13.218576431274414, rank=19, decoded_token=' upon'), 2425: Logprob(logprob=-13.406076431274414, rank=20, decoded_token=' under')}, {2: Logprob(logprob=-0.7334949970245361, rank=1, decoded_token='</s>'), 30259: Logprob(logprob=-1.9834949970245361, rank=2, decoded_token=' Prior'), 20218: Logprob(logprob=-2.108494997024536, rank=3, decoded_token=' Before'), 9380: Logprob(logprob=-2.858494997024536, rank=4, decoded_token=' Here'), 66224: Logprob(logprob=-3.233494997024536, rank=5, decoded_token=' Biden'), 2182: Logprob(logprob=-3.233494997024536, rank=6, decoded_token=' He'), 6304: Logprob(logprob=-3.858494997024536, rank=7, decoded_token=' His'), 3367: Logprob(logprob=-4.233494758605957, rank=8, decoded_token=' If'), 1531: Logprob(logprob=-4.295994758605957, rank=9, decoded_token=' The'), 3886: Logprob(logprob=-4.483494758605957, rank=10, decoded_token=' However'), 80033: Logprob(logprob=-4.545994758605957, rank=11, decoded_token=' Previously'), 3316: Logprob(logprob=-4.920994758605957, rank=12, decoded_token=' Is'), 41603: Logprob(logprob=-5.045994758605957, rank=13, decoded_token=' Previous'), 14424: Logprob(logprob=-5.108494758605957, rank=14, decoded_token=' Joe'), 10349: Logprob(logprob=-5.858494758605957, rank=15, decoded_token=' President'), 3213: Logprob(logprob=-6.108494758605957, rank=16, decoded_token=' You'), 3075: Logprob(logprob=-6.108494758605957, rank=17, decoded_token=' How'), 13980: Logprob(logprob=-6.170994758605957, rank=18, decoded_token=' Please'), 5675: Logprob(logprob=-6.295994758605957, rank=19, decoded_token=' What'), 3870: Logprob(logprob=-6.420994758605957, rank=20, decoded_token=' To')}], finish_reason=stop, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df783767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEPR Score: 0.8100503566291674\n",
      "EPR Score: 0.9999999999996603\n",
      "WEPR Token-level Scores: [0.99790025 0.55006045 0.05929238 0.03018122 0.03145245 0.03145734\n",
      " 0.03169614 0.02835398 0.13182984 0.03610038 0.0304431  0.03156283\n",
      " 0.02588933 0.04263151 0.05623298 0.13289703 0.02965864 0.02741074\n",
      " 0.03178211 0.03148381 0.03145764 0.03131408 0.03088118 0.03147761\n",
      " 0.03147975 0.03148025 0.03146403 0.03129065 0.06211783 0.9062832 ]\n",
      "EPR Token-level Scores: [3.0561693e+00 2.0080497e+00 1.0562013e+00 7.7853334e-01 4.6324534e-03\n",
      " 1.0769630e-02 6.4461357e-03 1.6148485e-01 1.9778041e+00 3.6181253e-01\n",
      " 2.1605838e-02 1.4578647e-02 1.6165379e-01 8.1536806e-01 4.7030860e-01\n",
      " 1.6716672e+00 2.9711109e-02 7.3007055e-02 1.6918281e-02 3.0829341e-04\n",
      " 4.4105260e-04 2.3310145e-03 6.8612481e-03 2.9114416e-04 4.5976598e-05\n",
      " 3.4797147e-05 3.8942546e-04 2.3475820e-03 1.1031432e+00 2.4985414e+00]\n"
     ]
    }
   ],
   "source": [
    "# Compute scores for the single output\n",
    "# The compute method expects a list of outputs or a single output object\n",
    "# It returns a list of scores\n",
    "\n",
    "wepr_scores = wepr.compute(outputs)\n",
    "epr_scores = epr.compute(outputs)\n",
    "\n",
    "epr_token_level_scores = epr.compute_token_scores(outputs)\n",
    "wepr_token_level_scores = wepr.compute_token_scores(outputs)\n",
    "\n",
    "wepr_score = wepr_scores[0]\n",
    "epr_score = epr_scores[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e501eccc",
   "metadata": {},
   "source": [
    "## 4. Results Analysis\n",
    "\n",
    "We analyze the scores to determine if the response is likely a hallucination.\n",
    "Threshold is 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5788ca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Who is the president of the United States?\n",
      "Response:  The current president of the United States is Joseph R. Biden Jr. He took office on January 20, 2021.\n",
      "------------------------------\n",
      "WEPR Score: 0.8101\n",
      "EPR Score:  1.0000\n",
      "------------------------------\n",
      "Prediction: HALLUCINATION (Uncertain)\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "\n",
    "if wepr_score > threshold:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artefactual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
